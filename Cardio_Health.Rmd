---
title: "Cardiovascular Health"
author: "Robert Castillo"
date: '2022-11-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading the Data & Libraries

```{r, echo = FALSE}
library(dplyr)
library(effects)
library(car)
library(randomForest)
library(readr)
library(ggplot2)
library(DAAG)
library(RColorBrewer)
library(corrplot)
library(Hmisc)
library(sjPlot)
library(caret)
library(alr4)
library(caTools)
library(ROCR)
library(pROC)
library(ROSE)
```

# EDA

```{r}
data_all <- read.csv("framingham.csv")

n_rows <- nrow(data_all)
n_na <- sum(!complete.cases(data_all))

cat('Percentage of rows with missing values: ',
     n_na/n_rows)

coul <- brewer.pal(5, "Set2") 

```

About 13% of the rows have missing values. We can remove those rows or impute the data with regression/nearest neighbor.

```{r}
data <- na.omit(data_all)
```

## 1 - Summarize Data

```{r}
summary(data)
```

-   BPMeds: 1 if the patient was on blood pressure medication (categorical)
-   prevalentStroke: 1 if the patient has had a stroke (categorical)
-   PrevalentHyp: 1 if the patient was hypertensive (categorical)
-   diaBP: 1 if the patient had diabetes (categorical)
-   totChol : total cholesterol level (numerical)

## 2 - Feature Engineering

```{r}
hd <-data$TenYearCHD #hd means heartdisease
Age <- data$age
Gender<- data$male
Education <- data$education
Currentsmoker<- data$currentSmoker

data$male[data$male==0] <-"female"
data$male[data$male==1] <-"male"

hd[hd==0] <- "Has No Heart Disease"
hd[hd==1] <- "Has Heart Disease"

table(hd) 
```

```{r}
categorical_variables <- all_of(c('currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes'))

for(col in categorical_variables) {
  data[col][data[col] == 1] <- 'Yes'
  data[col][data[col] == 0] <- 'No'
}

```

## 3 - Histograms

##### a. Gender Frequency

```{r}
#Gender
data$male[data$male==0] <-"female"
data$male[data$male==1] <-"male"

gender_table<- addmargins(table(data$male))
gender_table

ggplot(data, aes(factor(data$male),  fill= factor(data$male)))+geom_bar()+ theme_classic() +labs(title=("Gender Count"))
```

##### b. Age Frequency

```{r}
#Age

hist(data$age,  main = "Histogram of Age Frequency")

AgeGroup <- cut(data$age, c(30, 40, 50, 60, 70))
levels(AgeGroup) <- c("30's", "40's", "50's", "Above 60's")

barplot(table(AgeGroup), col=coul,main = "Frequency of Age Group")
```

##### c. Education Level Frequency

```{r}
#Education

data$education[data$education==1] <-"less than hs"
data$education[data$education==2] <-"hs grads"
data$education[data$education==3] <-"college grads"
data$education[data$education==4] <-"post college grads"
ed<- data$education
ed_table <- table(data$education)
addmargins(ed_table)
  
# to sort the education levels in an order that makes sense instead of alphabetical,
# we will use factor.
# First, we will create a level

education_levels <- c("less than hs", "hs grads", "college grads", "post college grads")
ed_leveled <- factor (ed, levels =education_levels)

barplot(table(ed_leveled), col= coul, main = "Education Levels Frequency")
```

##### d. Current SmokerFrequency

```{r}

currentsmoker<- data$currentSmoker

#Recoding the binary data  
currentsmoker[currentsmoker==0] <-"non-Smoker"
currentsmoker[currentsmoker==1] <-"Smoker"
table(currentsmoker)

barplot(table(currentsmoker), main="Histogram of Numbers of Current Smoker")
```

##### e. Cigs Per Day Frequency

```{r}

SmokerTypes<- cut(data$cigsPerDay, c(0, 5, 10, 20, 70, 90))
levels(SmokerTypes) <- c("Non-smoker", "light smoker", "casual smoker", "heavy smoker", "cigarette addict")
SmokerTypesTable<- table(SmokerTypes)
barplot(table(SmokerTypes), col=coul , main = "Cigarettes per Day")
```

##### f. BP Meds Frequency

```{r}
barplot(table(data$BPMeds),
        main='Patients on Blood Pressure Medication',
        names.arg=c("No", "Yes"))
```

##### g. Prevalent Strokes Frequency

```{r}
barplot(table(data$prevalentStroke),
        main='Patients with a history of strokes',
        names.arg=c("No", "Yes"))
```

##### h. Prevalent Hypertension Frequency

```{r}
barplot(table(data$prevalentHyp),
        main='Patients with hypertension',
        names.arg=c("No", "Yes"))

```

##### i. Stroke Frequency

```{r}
barplot(table(data$prevalentStroke),
        main='Patients with a history of strokes',
        names.arg=c("No", "Yes"))

```

##### h. DiaBP Frequency

```{r}
diabp <- data['diaBP']
diabp <- sapply(diabp, as.numeric)
ndiabp <- cut(diabp, c(0,80,89,max(diabp)))
levels(ndiabp) <- c('Normal', 'At risk', 'High Blood Pressure')
hist(diabp, main = 'Histogram of diastolic blood pressure')
barplot(table(ndiabp), main = 'Histogram of diastolic blood pressure')
```

##### j.  Syst BP Frequency

```{r}
sysbp <- data['sysBP']
sysbp <- sapply(sysbp, as.numeric)
nsysbp <- cut(sysbp, c(0,120,139,max(sysbp)))
levels(nsysbp) <- c('Normal', 'At risk', 'High Blood Pressure')
hist(sysbp, main = 'Histogram of systolic blood pressure')
barplot(table(nsysbp), main = 'Histogram of systolic blood pressure')
```

##### k. BMI

```{r}
bmi <- data['BMI']
bmi <- sapply(bmi, as.numeric)

nbmi <- cut(bmi, c(0,18.5,25,30,max(bmi)))
levels(nbmi) <- c('underweight', 'healthy weight', 'overweight', 'obesity')
hist(bmi, main = 'Histogram of BMI')
barplot(table(nbmi), main = 'Histogram of BMI by Levels')
```

##### l. Heart Rate

```{r}
heartrate <- data['heartRate']
heartrate <- sapply(heartrate, as.numeric)
hist(heartrate, main = 'Histogram of heart rate')
```

##### m. Glucose

```{r}
glucose <- data['glucose']
glucose <- sapply(glucose, as.numeric)
nglucose <- cut(glucose, c(0,99,125,max(glucose)))
levels(nglucose) <- c('normal', 'prediabetes', 'diabetes')
hist(glucose, main = 'Histogram of Glucose')
barplot(table(nglucose), main = 'Histogram of Glucose by Levels')
```

## 4- Checking for Outliers

```{r}
#systolic blood pressure

# get mean and Standard deviation
mean_sysbp = mean(data$sysBP)
std_sysbp = sd(data$sysBP)

# get threshold values for outliers
Tmin_sysbp = mean_sysbp-(3*std_sysbp)
Tmax_sysbp = mean_sysbp+(3*std_sysbp)

# find outlier
data$sysBP[which(data$sysBP < Tmin_sysbp | data$sysBP > Tmax_sysbp)]

#remove outlier
#data1$sysBP[which(data1$sysBP > Tmin_sysbp | data1$sysBP < Tmax_sysbp)]
```

```{r}
#diastolic blood pressure

# get mean and Standard deviation
mean_diabp = mean(data$diaBP)
std_diabp = sd(data$diaBP)

# get threshold values for outliers
Tmin_diabp = mean_diabp-(3*std_diabp)
Tmax_diabp = mean_diabp+(3*std_diabp)

# find outlier
data$diaBP[which(data$diaBP < Tmin_diabp | data$diaBP > Tmax_diabp)]
```

## 5 - Histograms between the predictors and the response variable

```{r}
#proportion of the people who had heart disease by Gender
gender_table <-table(data$male)
prop.table(gender_table)

gender_hd_table<- table(data$male,data$TenYearCHD)
gender_hd_proportion<- prop.table(gender_hd_table)
gender_hd_proportion

barplot(gender_hd_proportion, col=rainbow(2), legend.text = c("female", "male"), main="Proportion of Heart Disease by Gender")

barplot(gender_hd_proportion[,2], col=rainbow(2), main="People with Heart Disease by Gender")
```

Male seems to have slightly more heart disease.

```{r}
gender_hd_table1<- table(data$TenYearCHD,data$male)
gender_hd_table1
barplot(gender_hd_table1[2,], col=coul, main="Frequency of Heart Disease by Gender")
```

```{r}
#create another categorical column based on the age
data<- data%>% 
  mutate(AgeGroup = case_when(
age<40 & age>30 ~ "30s",
age>=40 & age<50 ~ "40s",
age>=50 & age<60 ~ "50s",
age>=60  ~ "above60s"
))

table(AgeGroup)
agegroup_table <- table(AgeGroup, data$TenYearCHD)
prop_agegroup <- prop.table(agegroup_table)


barplot(agegroup_table, col=coul, legend.text=c("30s", "40s", "50s", "above60s"), main="Frequencies of Heart Disease by Age Group")

barplot(prop_agegroup, col=coul, main="Heart Disease Proportion by Age Group")

barplot(prop_agegroup[,2], col=coul, main="Age Group with Heart Disease")
```

People in 50s seem to have heart disease the most.

```{r}
ed_table<-table(ed_leveled)
ed_table
prop.table(ed_table)

edhd<-table(ed_leveled, data$TenYearCHD)
edhd
edhd_proportion<-prop.table(edhd)

barplot(edhd_proportion[,2], col=coul, main="Proportion of People with Heart Disease by Education Levels")

#pink is the people with heart disease
```

We can see how less than high school groups has the highest proportion of having a heart disease.\

```{r}
#Current Smoker 
smokertable<- table(data$currentSmoker)
prop.table(smokertable)
smokerhd <- table(data$currentSmoker, data$TenYearCHD)
smoker_hd_prop<- prop.table(smokerhd)
smoker_hd_prop
smoker_hd_prop[,2]
barplot(smoker_hd_prop[,2], col=rainbow(2), main="Proportion of Smokers who Have Heart Disease")

```

Seems like there is barely any difference between current smoker in number of people with heart disease. Surprising. Maybe smoking at the moment is not the best indicator of predicting heart disease.\

```{r}
#Cigaretts Per Day
#create another categorical column based on types of smokers

data<- data%>% 
mutate(SmokerTypes = case_when(
cigsPerDay==0 ~"non-smoker",
cigsPerDay>0 & cigsPerDay<5 ~ "light smoker",
cigsPerDay>=5 & cigsPerDay<10 ~ "casual smoker",
cigsPerDay>=10 & cigsPerDay<20 ~ "heavy smoker",
cigsPerDay>=20 ~ "cigarett addict",
))

# View(data)

SmokerTypesTable
Smoker_HD<- table(SmokerTypes, data$TenYearCHD) 
Smoker_HD
proportion_smoke_hd <- prop.table(Smoker_HD)
proportion_smoke_hd 

barplot(proportion_smoke_hd[,2], col=coul, main="People with Heart Disease by Smoker Types")
```

```{r}
table(data$BPMeds)
bpmedstable<- table(data$BPMeds, data$TenYearCHD)
BPMeds_proportion<-prop.table(bpmedstable)
BPMeds_proportion

barplot(BPMeds_proportion, col=coul, legend.text=c("on Meds", "not on Meds"), main="Blood Pressure Medication and Heart Disease")

barplot(BPMeds_proportion[,2], col=coul, legend.text=c("Non on Meds", "on Meds"), main="Heart Disease Proportion Based on Blood Pressure Medication")
```

The second chart seems to show a majority of people with heart disease was NOT on blood pressure medication.

```{r}
data$diaBP <- ndiabp

diaBP_pro <- data %>% 
  group_by(diaBP) %>% 
  summarise(percent = n()/nrow(data))

barplot(as.matrix(data.frame(Normal = as.numeric(diaBP_pro['percent'][1,]), 
           At_risk = as.numeric(diaBP_pro['percent'][2,]),
           High_Blood_Pressure = as.numeric(diaBP_pro['percent'][3,]))), ylim=c(0,1),col=coul, main="Diatolic Blood Pressure Level vs. Heart Disease Proportion")
```

```{r}
boxplot(as.matrix(data.frame(Normal = as.numeric(diaBP_pro['percent'][1,]), 
           At_risk = as.numeric(diaBP_pro['percent'][2,]),
           High_Blood_Pressure = as.numeric(diaBP_pro['percent'][3,]))), ylim=c(0,1), )
```

```{r}
data$BMI <- nbmi

bmi_pro <- data %>% group_by(BMI) %>% summarise(percent = n()/nrow(data))

barplot(as.matrix(data.frame(underweight = as.numeric(bmi_pro['percent'][1,]), 
           healthy_weight = as.numeric(bmi_pro['percent'][2,]),
           overweight = as.numeric(bmi_pro['percent'][3,]), 
        obesity = as.numeric(bmi_pro['percent'][4,]))),ylim=c(0,1))
```

```{r}
data$diaBP <- ndiabp

diaBP_pro <- data %>% group_by(diaBP) %>% summarise(percent = n()/nrow(data))

diaBP_pro

barplot(as.matrix(data.frame(Normal = as.numeric(diaBP_pro['percent'][1,]), 
           At_risk = as.numeric(diaBP_pro['percent'][2,]),
           High_Blood_Pressure = as.numeric(diaBP_pro['percent'][3,]))), ylim=c(0,1))

```

## 6- Boxplots between the Numerical Variables and the Response

```{r}
boxplot(data$totChol~data$TenYearCHD, main="totChol vs. Heart Disease")
boxplot(data$age~data$TenYearCHD, main="Age vs. Heart Disease")
boxplot(data$cigsPerDay~data$TenYearCHD, main="Cigaretts Per Day vs. Heart Disease")
boxplot(data$sysBP~data$TenYearCHD,  main="Systolic Blood Pressure vs. Heart Disease")
boxplot(data$diaBP~data$TenYearCHD,  main="Diatolic Blood Pressure vs. Heart Disease")
boxplot(data$BMI~data$TenYearCHD,  main="BMI vs. Heart Disease")
boxplot(data$heartRate~data$TenYearCHD,  main="Heart Rate vs. Heart Disease")
boxplot(data$glucose~data$TenYearCHD,  main="Glucose vs. Heart Disease")
```

## 7- Check for correlation between categorical variables

```{r, echo = TRUE}
categorical_variables <- all_of(c('currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes'))
data %>% 
  select(categorical_variables) %>%
  summarise_all(funs(chisq.test(.,data$TenYearCHD)$p.value)) -> p_values
data %>% 
  select(categorical_variables) %>%
  summarise_all(funs(chisq.test(.,data$TenYearCHD)$statistic)) -> statistics

chisquare_output <- do.call(rbind, list(chi_squares=statistics, p_values=p_values))

```

## 8 - Interaction between variables

'currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes'

```{r}
# Stroke and hypertension history
glmodel1<-glm(TenYearCHD~prevalentStroke+prevalentHyp+prevalentStroke*prevalentHyp,
              data = data,
              family="binomial")
plot(allEffects(glmodel1), ask=FALSE)

# Diabetes history and BP meds
glmodel1<-glm(TenYearCHD~diabetes+BPMeds+diabetes*BPMeds,
              data = data,
              family="binomial")
plot(allEffects(glmodel1), ask=FALSE)

# Diabetes history and current smoker
glmodel1<-glm(TenYearCHD~currentSmoker+diabetes+currentSmoker*diabetes,
              data = data,
              family="binomial")
plot(allEffects(glmodel1), ask=FALSE)
```

## 9- Correlation Matrices of Numerical Variables

```{r}

data %>%select(age, cigsPerDay, totChol, sysBP, diaBP, BMI , heartRate, glucose)%>% summary()
data.mat <- as.matrix(data %>%
                       select(age, cigsPerDay, totChol, sysBP, diaBP, BMI , heartRate, glucose)%>%
                        mutate_all(as.numeric))
M <- rcorr(data.mat)
# M$r
corrplot(M$r, method = "number")

```

## 10. Residual Series Plot

```{r}
l<-glm(TenYearCHD ~ age + cigsPerDay + totChol + sysBP + diaBP + BMI + heartRate+glucose, data=data, family=binomial)


residualPlots(l)
```

## 11- Marginal Model Plot

```{r}
mmp(l)
```

## 12- VIF's \[Checking for Multicollinearity\]

```{r}
#A way to quantify the impact of multicollinearity is to look at the VIF (variance inflation factor). 

#A VIF close to 1 would imply no correlation. The larger the VIF the larger the amount of multicollinearity.

vif(l)
```


# Modeling and Analysis

## Load dataset, convert categorical features to factors, and add new features found from EDA

&nbsp;&nbsp;
```{r}
framingham_df <- na.omit(read.csv("framingham.csv"))


# convert original categorical variables to factors
categorical_variables <- c('TenYearCHD','education','currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes')
framingham_df[,categorical_variables] <- lapply(framingham_df[,categorical_variables], factor)


# rename factor levels of some original features 
levels(framingham_df$education) <- c("less than hs", "hs grads", "college grads", "post college grads")


# add engineered features 
framingham_df <- framingham_df %>% 
                 mutate(age_group = case_when(
                 age<40 & age>30 ~ "30s",
                 age>=40 & age<50 ~ "40s",
                 age>=50 & age<60 ~ "50s",
                 age>=60  ~ "above60s"
                 ))
   
framingham_df <- framingham_df %>% 
                 mutate(smoker_type = case_when(
                 cigsPerDay==0 ~"non-smoker",
                 cigsPerDay>0 & cigsPerDay<5 ~ "light smoker",
                 cigsPerDay>=5 & cigsPerDay<10 ~ "casual smoker",
                 cigsPerDay>=10 & cigsPerDay<20 ~ "heavy smoker",
                 cigsPerDay>=20 ~ "cigarett addict",
                 ))

diabp_cat <- cut(framingham_df$diaBP, c(0,80,89,max(framingham_df$diaBP)))
levels(diabp_cat) <- c('Normal', 'At risk', 'High Blood Pressure')
framingham_df$diabp_group <- diabp_cat

sysbp_cat <- cut(framingham_df$sysBP, c(0,120,139,max(framingham_df$sysBP)))
levels(sysbp_cat) <- c('Normal', 'At risk', 'High Blood Pressure')
framingham_df$sysbp_group <- sysbp_cat

bmi_cat <- cut(framingham_df$BMI, c(0, 18.5, 25, 30, max(framingham_df$BMI)))
levels(bmi_cat) <- c('underweight', 'healthy weight', 'overweight', 'obesity')
framingham_df$diabp_group <- bmi_cat

glucose_cat <- cut(framingham_df$glucose, c(0,98,max(framingham_df$glucose)))
levels(glucose_cat) <- c('Normal', 'Prediabetic / Diabetic')
framingham_df$diabp_group <- glucose_cat


# add significant numerical interactions from EDA 
framingham_df$cigs_and_sysbp <- framingham_df$cigsPerDay*framingham_df$sysBP
framingham_df$age_and_cigs <- framingham_df$cigsPerDay*framingham_df$age
framingham_df$gl_and_sysbp <- framingham_df$glucose*framingham_df$sysBP


# add potentially useful quadratic terms from EDA
framingham_df$sysBP_quad <- (framingham_df$sysBP)^2
framingham_df$diaBP_quad <- (framingham_df$diaBP)^2
framingham_df$glucose_quad <- (framingham_df$glucose)^2


# remove variables that were permanently replaced with categorical cuts
framingham_df = subset(framingham_df, select = -c(age, cigsPerDay) )

```

&nbsp;&nbsp;

## Logistic Regression Modeling w/o Interaction Features


### Backward Stepwise Feature Selection

&nbsp;&nbsp;

```{r}
glm_no_int1 <- glm(TenYearCHD ~ . - cigs_and_sysbp - age_and_cigs - gl_and_sysbp,
                   data=framingham_df, 
                   family=binomial)

summary(glm_no_int1)

# use backward selection
glm_no_int2 <- step(glm_no_int1, direction = "backward", trace=0) 
summary(glm_no_int2)


# difference in deviance can be used to compare the backward selection model
# and the original
anova(glm_no_int2, glm_no_int1, test="Chisq")

```

&nbsp;&nbsp;

From the above test, we can see that the model with the features selected from the backward selection seems to be sufficient (i.e. # there was no significant difference in original model with extra terms, so we use the simpler model (model_no_int2)). 

### Goodness of fit and Variance Explained Measures

We now check for goodness of fit, and examine the pseudo R^2 value of the simpler model.

&nbsp;&nbsp;


```{r}

best_LR_no_int <- glm(TenYearCHD ~ male + prevalentStroke + prevalentHyp + 
                      totChol + sysBP + diaBP + glucose + age_group + smoker_type + 
                      diaBP_quad, 
                      data = framingham_df,
                      family = binomial)
summary(best_LR_no_int)

# Goodness of Fit tests to be used with caution (binomial group sizes are small)

# test goodness of fit (pearson)
chi_stat <- sum(residuals(best_LR_no_int, type = "pearson")^2)
print(paste("Pearson Goodness of Fit p-value =", 1 - pchisq(chi_stat, 3640)))


# test goodness of fit (residual deviance vs null deviance)
dev_diff <- 3120.5  - 2758.5
df_diff <- 3655 - 3640
print(paste("Deviance Goodness of Fit p-value =", 1 - pchisq(dev_diff, df_diff)))


# pseudo R^2
pRsq <- 1 - (2758.5/3120.5)
print(paste("pseudo-R^2 = ", pRsq))



```

We can see from the summary that the AIC is 2790.5.

Since the calculated value of Pearson Chi square has large p-value, we fail to reject the null hypothesis, and conclude that the model fits the data well. Also, the small p-value for the Deviance Goodness of Fit test indicates that our model has less error than intercept only model and explains some of the variance in the outcome variable.

The pseudo R^2 is about 12%. However, that this pseudo R^2 metric is not very effective in measuring the varaince explained in many logistic regression settings.


### Influence & Outlier, Residual Analysis, and Goodness of Fit (MMP) Plots

```{r, warning=FALSE}
plot(best_LR_no_int)
influenceIndexPlot(best_LR_no_int, vars=c("Cook","hat"), id.n=3)

residualPlot(best_LR_no_int)
#residualPlots(best_LR_no_int)

mmp(best_LR_no_int)
#mmps(best_LR_no_int)

```


Although some hat-values exceed the conventional $2 \cdot\frac{2p}{n}$ (where our p is 10 predictors here) and n is the number of used observations, there are no large Std. Pearson Residuals as leverage gets larger. Most of the Std. Pearson Residuals remain relatively small throughout the entire range of hat values. This indicates that we mostly likely do not have any bad leverage (or influential) points.

Recall that the guidelines commonly used for Cook's distance is:

If $D_i$ is greater than 0.5, then the ith data point is worthy of further investigation as it may be influential.
If $D_i$ is greater than 1, then the $i$th data point is quite likely to be influential.
Or, if $D_i sticks out like a sore thumb from the other $D_i$ values, it is almost certainly influential.


Thus, we can see above that no Cook's distance is over 0.5 (all values are less than about 0.01), and no Cook's Distance really stick out terribly over the entire set of values. Maybe $i=3972$, however, it doesn't seem to stick out substantially compared to the rest of the values of the set, so we do not remove it from our model.

We can see above that the average pearson residual remains sufficiently close to 0 throughout the entire range of the linear predictor values, and throughout the entire range of each predictor variable independently, indicating a good fit for our data.

Additionally, the Marginal Model Plot displays a *very* close fit between the data and our model (the probability of having 10 year risk of coronary heart disease) across the entire range of the linear predictor values, and throughout the entire range of each numerical predictor variable independently. This indicates that this logistic model without interaction, is a very good fit for the data.


### 10-fold Cross Validation For Accuracy and AUC


&nbsp;&nbsp;

```{r}
set.seed(7)

train.control <- trainControl(method = "cv", number = 10)

cv_best_glm1 <- train(TenYearCHD ~ male + prevalentStroke + prevalentHyp + 
                      totChol + sysBP + diaBP + glucose + age_group + smoker_type + 
                      diaBP_quad, 
                      data = framingham_df, 
                      method = "glm",
                      family = "binomial",
                      trControl = train.control)
print(cv_best_glm1)

```
&nbsp;&nbsp;


The above 10-Fold Cross Validation gives us an estimated 85.2% test accuracy. This demonstrates good predictive performance on unseen data for the logistic regression model without interaction features.


## Logistic Regression Modeling w/ Interaction Features


### Backward Stepwise Feature Selection

&nbsp;&nbsp;

```{r}
glm_w_int1 <- glm(TenYearCHD ~ . + currentSmoker*diabetes + education*male,
                   data=framingham_df, 
                   family=binomial)

summary(glm_w_int1)

# use backward selection
glm_w_int2 <- step(glm_w_int1, direction = "backward", trace=0) 
summary(glm_w_int2)


# difference in deviance can be used to compare the backward selection model
# and the original
anova(glm_w_int2, glm_w_int1, test="Chisq")

```

&nbsp;&nbsp;

From the above test, we can see that the model with the features selected from the backward selection seems to be sufficient (i.e. # there was no significant difference in original model with extra terms, so we use the simpler model (model_w_int2)). Note, the feature selection process dropped all of the the categorical interaction terms.

### Goodness of fit and Variance Explained Measures

We now check for goodness of fit, and examine the pseudo R^2 value of the simpler model.

&nbsp;&nbsp;


```{r}

best_LR_w_int <- glm(TenYearCHD ~ male + prevalentStroke + prevalentHyp + 
                     totChol + sysBP + diaBP + glucose + age_group + age_and_cigs + 
                     diaBP_quad, 
                     family = binomial, 
                     data = framingham_df)
summary(best_LR_w_int)

# Goodness of Fit tests to be used with caution (binomial group sizes are small)

# test goodness of fit (pearson)
chi_stat <- sum(residuals(best_LR_w_int, type = "pearson")^2)
print(paste("Pearson Goodness of Fit p-value =", 1 - pchisq(chi_stat, 3643)))


# test goodness of fit (residual deviance vs null deviance)
dev_diff <- 3120.5  - 2758.2
df_diff <- 3655 - 3643
print(paste("Deviance Goodness of Fit p-value =", 1 - pchisq(dev_diff, df_diff)))


# pseudo R^2
pRsq <- 1 - (2758.2/3120.5)
print(paste("pseudo-R^2 = ", pRsq))



```

We can see from the summary that the AIC is 2784.2.

Since the calculated value of Pearson Chi square has large p-value, we fail to reject the null hypothesis, and conclude that this model also fits the data well. Also, the small p-value for the Deviance Goodness of Fit test indicates that our model has less error than intercept only model and explains some of the variance in the outcome variable.

The pseudo R^2 is also about 12%. However, that this pseudo R^2 metric is not very effective in measuring the varaince explained in many logistic regression settings.


### Influence & Outlier, Residual Analysis, and Goodness of Fit (MMP) Plots

```{r, warning=FALSE}
plot(best_LR_w_int)
influenceIndexPlot(best_LR_w_int, vars=c("Cook","hat"), id.n=3)

residualPlot(best_LR_w_int)
#residualPlots(best_LR_w_int)

mmp(best_LR_w_int)
#mmps(best_LR_w_int)

```


Although some hat-values exceed the conventional $2 \cdot\frac{2p}{n}$ (where our p is 15 predictors here) and n is the number of used observations, there are no large Std. Pearson Residuals as leverage gets larger. Most of the Std. Pearson Residuals remain relatively small throughout the entire range of hat values. This indicates that we mostly likely do not have any bad leverage (or influential) points.

Recall that the guidelines commonly used for Cook's distance is:

If $D_i$ is greater than 0.5, then the ith data point is worthy of further investigation as it may be influential.
If $D_i$ is greater than 1, then the $i$th data point is quite likely to be influential.
Or, if $D_i sticks out like a sore thumb from the other $D_i$ values, it is almost certainly influential.


Thus, we can see above that no Cook's distance is over 0.5 (all values are less than about 0.02), and no Cook's Distance really stick out terribly over the entire set of values. Maybe $i=3972$, however, it doesn't seem to stick out substantially compared to the rest of the values of the set, so we do not remove it from our model.

We can see above that the average pearson residual remains sufficiently close to 0 throughout the entire range of the linear predictor values, and throughout the entire range of each predictor variable independently, indicating a good fit for our data.

Additionally, the Marginal Model Plot displays a *very* close fit between the data and our model (the probability of having 10 year risk of coronary heart disease) across the entire range of the linear predictor values, and throughout the entire range of each numerical predictor variable independently. This indicates that this logistic model with interactions, is a very good fit for the data.


### 10-fold Cross Validation For Accuracy and AUC


&nbsp;&nbsp;

```{r}
set.seed(7)

train.control <- trainControl(method = "cv", number = 10)

cv_best_glm2 <- train(TenYearCHD ~ male + prevalentStroke + prevalentHyp + 
                      totChol + sysBP + diaBP + glucose + age_group + age_and_cigs + 
                      diaBP_quad, 
                      data = framingham_df, 
                      method = "glm",
                      family = "binomial",
                      trControl = train.control)
print(cv_best_glm2)

```
&nbsp;&nbsp;


The above 10-Fold Cross Validation gives us an estimated 85.1% test accuracy. This demonstrates good predictive performance on unseen data for the logistic regression model with interaction features. However, the simpler logistic regression model w/o interaction features had a similar (about 0.1% better) test accuracy. We will analyze and compare these two models with more detail in later sections.


## Random Forest Classifier for Baseline Comparison

&nbsp;&nbsp;

```{r}

rf_model <-randomForest(TenYearCHD ~ ., 
                        data=framingham_df,
                        ntree=500, 
                        mtry=8, 
                        importance = TRUE) 
print(rf_model)

```

&nbsp;&nbsp;



### Get Random Forest Estimate for Test Error Using 10 fold CV 

&nbsp;&nbsp;

```{r}

set.seed(7)

train.control <- trainControl(method = "cv", number = 10)

repGrid <- expand.grid(.mtry=c(8))  # standard default parameter

cv_rf <- train(TenYearCHD ~ ., 
                 data=framingham_df, 
                 method = "rf", 
                 trControl = train.control,
                 ntree=500, 
                 tuneGrid = repGrid)
  
print(cv_rf)

```

&nbsp;&nbsp;


### Feature Importances with Random Forest Model

&nbsp;&nbsp;

```{r}

# output the importances
importance(rf_model)

# let's save the varImp object
imp <- varImpPlot(rf_model) 

# this part just creates the data.frame for the plot part
imp <- as.data.frame(imp)
imp$varnames <- rownames(imp) # row names to column
rownames(imp) <- NULL
names(imp)[1] <- "MeanDecreaseAccuracy"
# this is the plot part, be sure to use reorder with the correct measure name
ggplot(data=imp, aes(x=reorder(varnames, +MeanDecreaseAccuracy), y=MeanDecreaseAccuracy, fill=varnames)) +
geom_bar(stat = "identity") +
coord_flip() +
theme(legend.position="none") +
scale_fill_discrete(name="Variable Group") +
ylab("MeanDecreaseAccuracy") + 
xlab(" ")

```

&nbsp;&nbsp;

### Convert remaining variables to factors
```{r}
framingham_df$male <- as.factor(framingham_df$male)
framingham_df$age_group <- as.factor(framingham_df$age_group)
framingham_df$smoker_type <- as.factor(framingham_df$smoker_type)
```

### ROC
```{r}
CHD_Table <- table(framingham_df$TenYearCHD)
prec_rec_baseline <- CHD_Table[2]/sum(CHD_Table)

# Split data
train.index <- createDataPartition(framingham_df$TenYearCHD, p=0.8, list = F)
train_data <- framingham_df[train.index,]
test_data <- framingham_df[-train.index,]

# Retrain Models
lr_model_no_int <- glm(TenYearCHD ~ male + prevalentStroke + prevalentHyp + 
                      totChol + sysBP + diaBP + glucose + age_group + smoker_type + 
                      diaBP_quad, 
                      data = train_data,
                      family = binomial)
lr_model_w_int <- glm(TenYearCHD ~ male + prevalentStroke + prevalentHyp + 
                     totChol + sysBP + diaBP + glucose + age_group + age_and_cigs + 
                     diaBP_quad, 
                     family = binomial, 
                     data = train_data)
random_forest_model <- randomForest(TenYearCHD ~ ., 
                        data=train_data,
                        ntree=500, 
                        mtry=8, 
                        importance = TRUE)

result_no_int <- predict(lr_model_no_int,newdata=test_data, type = "response")
result_w_int <- predict(lr_model_w_int, newdata = test_data, type = "response")
result_rf <- predict(random_forest_model,newdata = test_data, type = "prob")



pred_no_int <- prediction(result_no_int,test_data$TenYearCHD)
pred_w_int <- prediction(result_w_int,test_data$TenYearCHD)
pred_rf <- prediction(result_rf[,2],test_data$TenYearCHD)


acc_no_int <- performance(pred_no_int,"acc")
acc_w_int <- performance(pred_w_int,"acc")
acc_rf <- performance(pred_rf,"acc")

plot(acc_no_int)
plot(acc_w_int)
plot(acc_rf)

roc_curve_no_int <- performance(pred_no_int,"tpr","fpr")
roc_curve_w_int <- performance(pred_w_int,"tpr","fpr")
roc_curve_rf <- performance(pred_rf,"tpr","fpr")

prec_rec_curve_no_int <- performance(pred_no_int,"prec","rec")
prec_rec_curve_w_int <- performance(pred_w_int,"prec","rec")
prec_rec_curve_rf <- performance(pred_rf,"prec","rec")

plot(roc_curve_no_int,col = "blue")
plot(roc_curve_w_int, add = TRUE, col = "red")
plot(roc_curve_rf,add = TRUE, col = "green") + abline(0,1)


plot(prec_rec_curve_no_int,col = "blue")
plot(prec_rec_curve_w_int, add = TRUE, col = "red")
plot(prec_rec_curve_rf, add = TRUE, col = "green") + abline(prec_rec_baseline,0)

```

**Analysis: ** Based on the ROC curve above, it seems that the models with and without interaction are pretty comparable, although both perform better than the random Forest model.

### Confusion Matrices
```{r}
# No Interaction Model

no_int_threshold <- as.factor(as.numeric(result_no_int>.45))
no_int_table <- table(test_data$TenYearCHD, no_int_threshold)
confusionMatrix(no_int_table)

# With Interaction Model
w_int_threshold <- as.factor(as.numeric(result_w_int>.45))
w_int_table <- table(test_data$TenYearCHD, w_int_threshold)
confusionMatrix(w_int_table)

# RF Model
rf_threshold <- as.factor(as.numeric(result_rf[,2]>.45))
rf_table <- table(test_data$TenYearCHD, rf_threshold)
confusionMatrix(rf_table)

```

**Analysis:** We set the cutoff for the models with and without interaction at .5. While it does not maximize accuracy or sensitivity, it creates a good balance between the two. For the model without interaction the accuracy is about 67% and sensitivity is at 92%. For the model with interaction the accuracy is about 66% and sensitivity is at 94%. For the random forest model, we set the cutoff at .1. This results in 60% accuracy with 89% sensitivity. Based on the confusion matrices, it appears that either the model with interaction or the model without interaction could be used. However, since we are trying to predict the likelihood of having 10 year risk of CHD, sensitivity is the more important metric when determining which model to choose. As a result, it looks like choosing the model with the interaction is the better option since we are gaining 2% sensitivity for a 1% loss in accuracy. The random forest model has a lower accuracy and sensitivity, so it isn't as effective as the other two models.  

### Calculate AUC and Confidence Intervals
```{r}
# No Interaction Model
roc_no_int <- roc(test_data$TenYearCHD, result_no_int)
auc_no_int <- roc_no_int$auc
ci_no_int <- ci(roc_no_int, conf.level = 0.95)
lower_no_int <- ci_no_int[1]
mean_no_int <- ci_no_int[2]
upper_no_int <- ci_no_int[3]
cat("Model With No Interaction AUC: ",auc_no_int,"\n")
cat("Model With No Interaction CI: [", lower_no_int, ", ", upper_no_int,"]")

# With Interaction Model
roc_w_int <- roc(test_data$TenYearCHD, result_w_int)
auc_w_int <- roc_w_int$auc
ci_w_int <- ci(roc_w_int, conf.level = 0.95)
lower_w_int <- ci_w_int[1]
mean_w_int <- ci_w_int[2]
upper_w_int <- ci_w_int[3]
cat("Model With Interaction AUC: ",auc_w_int,"\n")
cat("Model With Interaction CI: [", lower_w_int, ", ", upper_w_int,"]")

# RF Model
roc_rf <- roc(test_data$TenYearCHD, result_rf[,2])
auc_rf <- roc_rf$auc
ci_rf <- ci(roc_rf, conf.level = 0.95)
lower_rf <- ci_rf[1]
mean_rf <- ci_rf[2]
upper_rf <- ci_rf[3]
cat("Random Forest Model AUC: ",auc_rf,"\n")
cat("Random Forest Model CI: [", lower_rf, ", ", upper_rf,"]")

```

### Plot the AUC Confidence Intervals
```{r}
x <- c(mean_no_int, mean_w_int, mean_rf)
y <- c("No Interaction", "With Interaction", "Random Forest")
low <- c(lower_no_int, lower_w_int, lower_rf)
up <- c(upper_no_int, upper_w_int, upper_rf)

ci_df <- data.frame(x = x, y = y, low = low, up = up)
ggplot(ci_df, aes(x,y)) + geom_point() + geom_errorbar(aes(xmin = low, xmax = up)) + xlim(.6,1.00) + xlab("AUC") + ylab("Model")
```

**Analysis: ** Based on the graph above, the model with interaction performs a little bit better than the model with no interaction term. The mean of the AUC confidence interval for the model with interaction is .756 and the mean of the AUC confidence interval for the model without interaction is .751. There is not much of a difference between the two models, but both models perform better than the random forest model. The mean of the AUC confidence interval for the random forest model is .691.

### Odds Ratios, CI, and P-values

```{r}
# With Interaction Model
w_int_summary <- summary(best_LR_w_int)
w_int_odds <- exp(coef(best_LR_w_int))
w_int_odds_ci <- exp(confint(best_LR_w_int))
w_int_pvalues <- w_int_summary$coefficients[,4]
w_int_matrix <- data.frame(w_int_odds, w_int_odds_ci, w_int_pvalues)
w_int_matrix <- w_int_matrix %>% rename(odds_ratio = w_int_odds, X2.5 =  X2.5.., X97.5 = X97.5.., pvalues = w_int_pvalues)
w_int_matrix
```

### Plot Odds Ratios and Confidence Intervals

```{r}
#With Interaction
plot_model(best_LR_w_int)
```

**Analysis:** The interpretation of the odds ratios are as follows:  
1) Compared to females, males are 78% more likely to have 10 year risk of CHD
2) Compared to those who have not previously had a stroke, those who previously had a stroke are 2.13 times more likely to have 10 year risk of CHD  
3) Compared to those who are not hypertensive, those who are hypertensive are 37% more likely have 10 year risk of CHD.  
4) As Total Cholesterol level increases by one unit, the odds of having 10 year risk of CHD does not change in any significant way .  
5) As systolic blood pressure increases by one unit, the odds of having 10 year risk of CHD increases by 1.6%.  
6) As Diastolic Blood pressure increases by one unit, the odds of having 10 year risk of CHD decreases by about 10%.  
7) As Glucose level increases by one unit, the odds of having 10 year risk of CHD does not change in any significant way.  
8) Compared to those aged in their 30s, those aged in their 40s are 98% more likely to have 10 year risk of CHD.  
9) Compared to those aged in their 30s, those aged in their 50s are 3.8 times more likely to have 10 year risk of CHD.  
10) Compared to those aged in their 30s, those aged in their 60s and above are 5.19 times more likely to have 10 year risk of CHD.  
11) As age increases by one year and cigarettes per day increases by one cigarette, the odds of having 10 year risk of CHD does not change in any significant way. 
12) As Diastolic Blood pressure squared increases by one unit, the odds of 10 year risk of CHD does not change in any significant way.

```{r}
# 10 unit increase in interaction age_cigs
exp(.0003572*10)

# 10 unit increase in diaBP_quad
exp(.0005612*10)
```

